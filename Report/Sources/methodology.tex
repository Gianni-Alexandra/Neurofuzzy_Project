% !TeX spellcheck = en_US
\section{Methodology}

To construct a robust multi-class text classifier, our methodology was meticulously designed to ensure both efficiency and accuracy. The process is segmented into distinct phases, as shown below.

\subsection{Import Library}
When embarking on the implementation of a machine learning project, such as the development of a multiclass text classifier, the first step involves setting up the computational environment by importing the necessary libraries. These libraries provide pre-written functions and classes that facilitate data manipulation, model building, training, and evaluation, significantly reducing the amount of code we need to write from scratch and making our job easier.

\begin{lstlisting}
# Interact with Operation System
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# Load, explore and plot data
import string
import numpy as np
import tensorflow as tf
import pandas as pd
import re
import ast

# Train test split
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from collections import Counter
from imblearn.over_sampling import RandomOverSampler

# Modeling
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import LSTM
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import GlobalMaxPooling1D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LeakyReLU
\end{lstlisting}

These are all the libraries required to load, test, train and build our model. To have a better understanding about their functionality, we will briefly discuss them.
\begin{itemize}
	\item \textbf{os}: A standard Python library for interacting with the operating system. It's being used here to set an environment variable
	
	\item \textbf{nltk}: The Natural Language Toolkit, or NLTK, is a library used for working with human language data.
	
	\item \textbf{numpy}: A basic Mython library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.
	
	\item \textbf{tensorflow}: An open-source software library for machine learning and artificial intelligence. It provides a flexible platform for defining and running computations that involve tensors, which are partial derivatives of a function with respect to its variables. This is the main library that we use in order to train and validate the text classifier.
	
	\item \textbf{pandas}: A software library for data manipulation and analysis. It provides data structures and functions needed to manipulate structured data.
	
	\item \textbf{re}: This module provides regular expression matching operations
	
	\item \textbf{sklearn}: Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms.
	
	\item \textbf{collections}: This module implements specialized container datatypes providing alternatives to Pythonâ€™s general purpose built-in containers.
	
	\item \textbf{imblearn}: A Python library to tackle the curse of imbalanced datasets in machine learning.
\end{itemize}

\subsection{Data Acquisition}
The initial step involved getting the dataset provided by the professor and understanding its contents. The \verb|news-classification.csv| file is a Comma-Separated Values file, a common file type for distributing large amounts of data over the internet. 
This type of data type can be viewed as a large array of structs that contain a lot of information, but we only need the following columns:
\begin{itemize}
	\item \verb|category_level_1|: Name of text's category (\textit{strings}).
	\item \verb|category_level_2|: Name of text's subcategory (\textit{strings}).
	\item \verb|content|: The actual text content (\textit{strings}).
\end{itemize}
The rest of the columns are not necessary because they do not give us some kind of important information about the text's contents.

As we are using Python for this project, in order to load this CSV file into memory, we used \verb|pandas|'s \verb|read_csv()| function that automatically imports the necessary file to a \verb|Dataframe| format.

\subsection{Data Cleaning}

Text is just a sequence of words, or more accurately, a sequence of characters. However, when we are usually dealing with language modelling or natural language processing, we are more concerned about the words as a whole rather than focusing only on the character-level depth of our text data. One explanation for this is the lack of \say{context} for individual characters in the language models.\\
The moment data are imported into the RAM, preparation begins in order to transform the text from human to machine understandable. 
First of all, lower casing of all the letters is very important and used for better handling of the file.
Everything inside the content array that doesn't give enough information can be considered noise and needs to be removed.
A great example of \say{noise} is:
\begin{itemize}
	\item URLs,
	\item Email addresses,
	\item Lines like \say{This post was published on the site} (\textit{which can be often found at the start of an article}),
	\item Multiple space or new line characters,
	\item Punctuation.
\end{itemize}  

In the preprocessing phase of text classification, one critical step is the removal of stopwords, which are words that do not contribute significant meaning to the text and are thus considered irrelevant for analysis.
The Natural Language Toolkit (NLTK), a comprehensive library for natural language processing in Python, provides an extensive dictionary of stopwords across multiple languages. Utilizing NLTK's stopwords dictionary allows for the efficient filtering out of common words such as "the", "is", "in", and "and", which appear frequently in text but do not carry substantial information relevant to the classification task.
The process involves iterating over the words in the dataset and removing those that are present in the NLTK stopwords list.
This reduction in dataset size not only streamlines the computational process by focusing on words that carry more meaning but also improves the model's ability to learn by concentrating on content that is more likely to influence the classification outcome.

\subsection{Dataset Split}
In the development of this text classifier, a critical step in the methodology is the partitioning of the dataset into training and validation subsets.
This process, essential for both training the model effectively and evaluating its performance, employs a standard split ratio of $80\%$ for training data and $20\%$ for validation data.
Such a division is strategically chosen to provide the model with a sufficiently large training dataset, enabling it to learn the underlying patterns of the text, while also reserving a representative portion of the data for performance evaluation and tuning.
The use of a validation set, separate from the training set, is pivotal in detecting and mitigating overfitting, ensuring that the model generalizes well to new, unseen data.
Also, the selected ratio we chose is very popular in bibliography and on the internet as well.

To facilitate this data partitioning, we utilize the \verb|train_test_split| function provided by the \verb|sklearn| library, a tool renowned for its robustness and ease of use in the machine learning community. This function streamlines the process of randomly dividing the dataset according to the specified proportions, ensuring that the split is both efficient and reproducible. By leveraging this method, we can maintain the integrity of the data's distribution, ensuring that both the training and validation sets are representative of the overall dataset. This approach not only simplifies the preprocessing workflow but also lays a solid foundation for the subsequent training phase, enabling a systematic and controlled development of a high-performing text classification model.

\subsection{Handling of Class Imbalance}
During the text cleaning and preparation procedure, we noticed that there's a big class imbalance that affected the outcome of our classifier. Figure \ref{fig:initial_class_distribution} shows that \verb|lifestyle| class has the lower appearance in the set and that means our classification system cannot detect it easily.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\linewidth]{Images/initial_class_distribution.pdf}
	\caption{Initial class distribution of the dataset}
	\label{fig:initial_class_distribution}
\end{figure}

To address this issue in our dataset, we utilized the \verb|RandomOverSampler| method provided by the \verb|imblearn| library. This technique involves artificially augmenting the under-represented classes in the training set by randomly replicating instances until all classes achieve a similar size. By doing so, we ensure that the neural network does not become biased towards the more frequent classes and can learn the characteristics of all classes equally. This step is crucial for improving the model's ability to generalize well across the entire range of classes, particularly for those that are less represented in the original dataset.

In the implementation phase, \verb|RandomOverSampler| was applied after splitting the dataset into training and validation sets but before the model training process. This sequencing is intentional to prevent the oversampling process from influencing the validation set, thereby maintaining its integrity as a representative sample of real-world data. The application of \verb|RandomOverSampler| is straightforward, thanks to the intuitive API of \verb|imblearn|. With a few lines of code, we were able to fit the sampler to our training data, resulting in a modified training set with balanced class distributions. The distribution after balancing can be shown in figure \ref{fig:final_class_distribution}.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\linewidth]{Images/final_class_distribution.pdf}
	\caption{Final class distribution of the given dataset}
	\label{fig:final_class_distribution}
\end{figure}
