% !TeX spellcheck = en_US
\section{Results}
The model created earlier is trained and then tested on separate data with the code in the following snippet:
\begin{lstlisting}[language=Python]
# Training
history = model.fit(train_texts, train_labels, validation_data=(test_texts, test_labels), epochs=epochs, batch_size=batch_size, use_multiprocessing=True)

# Evaluation
loss, accuracy = model.evaluate(test_texts, test_labels)
\end{lstlisting}

\subsection{Performance}

The evaluation of our multiclass text classifier has provided insightful observations into its performance, with the network achieving an overall accuracy of around $78\%$ using \verb|category_level_1| labels as shown in Figure~\ref{fig:epoch_vs_accuracy_level_1}.
This metric signifies that $\approx 3/4$ of the text samples were classified correctly across the various classes.\\
While this demonstrates a solid foundation in the classifier's ability to discern and categorize text data accurately, it also indicates room for improvement.
An accuracy of $78\%$ suggests that, although the classifier is generally reliable, there are instances where it may struggle to correctly identify the class labels.
This level of performance sets a benchmark for future iterations of the model, where enhancements can be targeted towards reducing the classification errors and increasing the accuracy.

Upon evaluating our multiclass text classifier with \verb|category_level_2|, we observed a marked decrease in performance, with the model achieving an overall accuracy of just $50\%$ as shown in Figure~\ref{fig:epoch_vs_accuracy_level_2}. This stark contrast from the previously discussed accuracy highlights the challenges inherent in text classification tasks, especially when dealing with diverse or more complex label sets. An accuracy of $50\%$ indicates that the classifier struggles significantly to correctly identify the class labels for a majority of the text samples. This performance level suggests that the classifier may be unable to capture the nuances and variations required to distinguish between the classes effectively in this particular label set. Such a result prompts a critical review of the classifierâ€™s design, feature extraction methods, and training process, signaling a need for substantial adjustments to improve its ability to generalize across different sets of labels.

We tried a number of different architectures and parameters, but we were unable to achieve a better outcome. We chose to maintain this architecture as the final one because, of course, there was also a time constraint.

\begin{figure}[htpb]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/level_1_epoch_accuracy.pdf}
		\caption{\textit{category\_level\_1}}
		\label{fig:epoch_vs_accuracy_level_1}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/level_2_epoch_accuracy.pdf}
		\caption{\textit{category\_level\_2}}
		\label{fig:epoch_vs_accuracy_level_2}
	\end{subfigure}
	\caption{Accuracy with respect to iterations number}
	\label{fig:epocs_vs_accuracy}
\end{figure}

\subsection{Parameters}

As far as the final parameters are concerned, we can not write each and every parameter here. The reason why can be clear once we print some insights about one of our models. After execution, we used \verb|print(model_level_1.summary())| and it returned the following:
\begin{lstlisting}[language=Python]
	_________________________________________________________________
	Layer (type)                Output Shape              Param #   
	=================================================================
	embedding_2 (Embedding)     (None, 6544, 17)          2207705   
	
	conv1d_2 (Conv1D)           (None, 6533, 512)         104960    
	
	global_max_pooling1d_2 (Gl  (None, 512)               0         
	obalMaxPooling1D)                                               
	
	batch_normalization_2 (Bat  (None, 512)               2048      
	chNormalization)                                                
	
	dropout_2 (Dropout)         (None, 512)               0         
	
	dense_2 (Dense)             (None, 17)                8721      
	
	=================================================================
	Total params: 2323434 (8.86 MB)
	Trainable params: 2322410 (8.86 MB)
	Non-trainable params: 1024 (4.00 KB)
	_________________________________________________________________
\end{lstlisting}
As we can see here, there is no way of printing all of the parameters that models use. Indicatively, some are listed in the table below:
\begin{table}[htpb]
	\centering
	\begin{tblr}{hlines, vlines, colspec={ccc}}
		\textbf{Layer} &\textbf{Weight/Bias} & \textbf{Value} \\
		Embedding & $embedding[0][0]$ & $0.01644432$ \\ 
		Conv & $weight[0][0][0]$ & $-0.04686261$ \\
		Dense & $weight[0][0]$ & $-0.15257788$ \\
		Dense & $bias[0]$ & $-0.01133604$ \\
	\end{tblr}
	\caption{Weights and biases of some layers}
\end{table}

\subsection{Metrics}

The final metrics of loss and accuracy, as depicted in Figure \ref{fig:classifier_final_metrics}, provide a clear, comparative view of the text classifier's performance across two different categories. From the visual data, it is apparent that the classifier achieves different levels of success when predicting each category. 

\begin{figure}[htpb]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/final_loss_categories.pdf}
		\caption{Loss}
		\label{fig:classifier_final_loss}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/final_accuracy_categories.pdf}
		\caption{Accuracy}
		\label{fig:classifier_final_accuracy}
	\end{subfigure}
	\caption{Final metrics of our text classifier}
	\label{fig:classifier_final_metrics}
\end{figure}


The loss metric, represented in Figure~\ref{fig:classifier_final_loss}, shows a lower value for the first category compared to the second. Since loss measures the model's errors in prediction, a lower loss indicates that the model is more precise in predicting the first category. This suggests that the features of the first category are likely more distinct or that the model has learned patterns that are more representative of this particular category.

Conversely, the accuracy metric, shown in Figure~\ref{fig:classifier_final_accuracy}, demonstrates a higher value for the first category, which implies that the classifier is more often correct when identifying instances of this category. Higher accuracy in this context means that the model has a better grasp of the characteristics that define the first category and is able to generalize well when making predictions on the test data.

\subsection{Training Time}

The training time of a neural network as a function of the size of the input data reflects how algorithm efficiency and computational demand change with increasing data volumes. Generally, as the input data size increases, the training time also increases due to the greater number of computations required. This relationship is crucial for understanding and optimizing the performance and scalability of our algorithm, especially when dealing with large datasets, such as on this project. Efficient data handling and model optimization techniques become essential to manage training time effectively.

We can clearly see from the Figure~\ref{fig:train_time} that the training time for the \verb*|category_level_2| is bigger than the training time of \verb*|category_level_1| and it is due to the higher computational workload \verb*|category_level_2| has to handle.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.7\textwidth]{Images/train_time_categories.pdf}
	\caption{Train\_time for both categories}
	\label{fig:train_time}
\end{figure} 

\subsection{Inference Time}
Inference time refers to the duration a machine learning model takes to make predictions on new, unseen data after being trained. Figure~\ref{fig:inference_time} shows detailed analysis and data on how long the neural network model takes to predict outcomes based on new, unseen data. This measurement is critical for assessing the model's efficiency in real-time applications, where faster inference times are crucial for user experience and system performance. We are comparing inference times under different conditions, such as varying input data sizes, model complexities, or computational resources, providing insights into how each factor influences the speed of making predictions.
Moreover, we can agree that the inference time of \verb*|category_level_2| is much bigger than the one of \verb*|category_level_1|. This happens because the input data sizes and the label sizes are much bigger. 

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.6\textwidth]{Images/eval_inference_time.pdf}
	\caption{Inference time for both categories}
	\label{fig:inference_time}
\end{figure} 
